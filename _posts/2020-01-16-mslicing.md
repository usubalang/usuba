---
layout: post
title: mslicing
date: "2020-01-16 00:00:00"
description: mslicing presentation
lang: en
locale: en_US
author: Darius Mercadier
excerpt: The previous post introduced bitslicing, a technique to speed up cipher implementations by introducing huge amounts of data-parallelism. However, this technique cannot be applied on ciphers relying on arithmetic operations, puts a lot of pressure on registers, and requires a lot of independent inputs to be efficient. In order to overcome those issues, we propose a generalization of bitslicing called mslicing. Msliced codes use much fewer registers than bitsliced ones, and can fully exploit the capabilities of SIMD extensions.
comments: true
hidden: false
---


<!--
Introduction
 - limitations bitslicing
   + parallelism
   + register pressure
   + arithmetics
 - K&S byteslice AES
   + 16x8 instead of 1x128
   + less register pressure
   + less parallelism
 - 2 data layouts
   + vslicing, allows arithmetics
   + hslicing allows permutations
-->

As shown in the [previous post]({{ site.baseurl }}{% post_url
2020-01-14-bitslicing %}), bitslicing can produce very
high-throughput cipher implementations through data-parallelism, but
suffers from a few limitations:

 - it requires _a lot_ of independent inputs to be efficient, since
   high throughput is achieved by encrypting different inputs in
   parallel.
   
 - it puts a lot of pressure on the registers, which causes spilling
   (moving data back-and-forth between registers and memory), thus
   reducing performances.

 - it cannot efficiently implement ciphers which rely heavily on
   arithmetic operations, like Chacha20 [1] and Threefish [2].


To overcome the first two issues, Kasper & Schwabe [3] proposed a
"byte-sliced" implementation of AES. Typical bitslicling would
represent the 128-bit block as 1 bit in 128 registers. Instead, they
represent the 128-bit block as 16 bits in 8 registers. Using only 8
registers greatly reduces register pressure and allows AES to be
implemented without any spilling. Furthermore, this representation
requires less inputs to fill the registers and thus maximize
throughput: on AES, only 8 parallel inputs are required to fill up
128-bit SSE registers (versus 128 with bitslicing).


We define _m_-slicing as a generalization of bitslicing, where a
_n_-bit input is split into _k_ bits in _m_ registers (such that _k *
m = n_). The special case _m = 1_ corresponds to bitslicing. When _k_
(the number of bits in each register) is greater than one, there are
two possible ways to arrange the bits within each register: they can
either be stored contiguously, or they can be splitted. The first
option, which we call _vslicing_, will enable the use of packed
instructions (_e.g._ 16-bit addition), while the second option, which
we call _hslicing_, will enable the use of permutation instructions
(_e.g._ `pshufb`). This post will explain in detail how each technique
works, and how they can be used to improve on bitslicing.


<!--
Vertical slicing
 - =~ vectorization?
 - bits are packed together
   * example: Rectangle (re-use slide from PLDI)
 - can use arithmetic operations
 - best implems of Serpent and Chacha use Vslicing
-->
## Vertical slicing

Rather than considering the bit as the atomic unit of computation, one
may use _m_-bit words as such basis (_m_ being a word size supported
by the SIMD architecture). On Intel (SSE/AVX/AVX512), _m_ could for
example be 8, 16, 32, or 64. We can then exploit _vertical_ _m_-bit
SIMD instructions to perform logic as well as arithmetic in
parallel. If we visually represent registers as aggregations of
_m_-bit elements vertically stacked, vertical operations consist in
element-wise computations along this vertical direction. Such
operations include additions, multiplications, shifts, rotations, as
well as traditional bitwise operations. For instance, the instruction
`paddb` takes two SSE registers, each seen as 16 8-bit stacked
elements, and computes the 16 8-bit additions between each one of
their elements:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/paddb-SSE-small.png">
</p>

Similarly, `pslld` takes an SSE register, seen as 4 32-bit stacked
elements, and an integer _n_, and left-shifts each element of the SSE
register by _n_:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/pslld-small.png">
</p>

Note that two _m_-bit elements of the same register never interact
together: there is no possbility to add them or to xor them for
instance.

This technique, called vertical slicing (or vslicing for short), is
similar to the notion of
[_vectorization_](https://en.wikipedia.org/wiki/Automatic_vectorization)
in the compilation world. Compared to bitslicing, it puts less
pressure on registers, and requires less parallel data to fill the
registers. Furthermore, it can be used to implement arithmetic-based
ciphers (like Chacha20), which cannot be done efficiently in
bitslicing.


However, vslicing is weaker than bitslicing when it comes to
permutations. Applying an arbitrary bit-permutation to the whole block
needs to be done manually using shifts and masks to extracts bits from
the registers and recombine them. By comparison, this could be done at
compile time with bitslicing. 


<!--
Nevertheless, for some ciphers, like
AES, where permutations can be done within each register elements (as
opposed to "between each 16-bit elements as well"), a better layout of
the data can be used to perform more efficient permutations. This
layout is the basis of horizontal slicing.
-->


#### Vsliced Rectangle


The Rectangle cipher [4] (presented [earlier]({{ site.baseurl }}{%
post_url 2020-01-07-usuba-genesis %})) encrypts a 64-bit input. We can
split it into 4 16-bit elements, and store them in the first 16-bit
packed elements of 4 SSE registers:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/vslicing_oneway_small.png">
</p>

Then, applying the same principle as bitslicing, we can fill the
remaining empty elements of those SSE registers with independent
inputs. The second input would go in the second packed elements of
each register:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/vslicing_twoway_small.png">
</p>

And so on until the registers are full (in the case of Rectangle on
128-bit SSE registers, this is achieved with 8 inputs).


Vslice Rectangle can be efficiently computed in parallel:

 - The Sbox is a sequence of bitwise `and`, `xor`, `not` and `or`
   between each of those SSE registers.
   
 - The permutation can be written as three left-rotations, each one
   operating on a different SSE registers. While the SSE extension
   does not offer a rotation instruction, it can be emulated using two
   shifts and an `or`. This is slightly more expensive than the
   bitsliced implementation of Rectangle, which can perform this
   permutation at compile time. However, the reduced register pressure
   more than compensate for this loss.




<!--
Horizontal slicing
 - cf example from previous paragraph
 - bits are splitted within the registers
 - still cannot use arithmetic
 - can use permutations
   + example? (eg, vpshufb on AES's shiftrows?)
 - best implementations of AES use hslicing
-->
## Horizontal slicing

Rather than considering an _m_-bit atom as a single packed element, we
may also dispatch its _m_ bits into _m_ distinct packed elements,
assuming that m is less or equal to the number of packed elements of
the architecture. Using this representation, we lose the ability to
perform arithmetic operations but gain the ability to perform
arbitrary shuffles of our _m_-bit word in a single instruction (for
instance using `pshufb` on SSE registers). This style relies on
horizontal SIMD operations: we can perform element-wise computations
within a single register, _i.e._ along an horizontal direction.

We call this technique _horizontal_ slicing, or hslicing for
short. Like vslicing, it lowers register pressure compared to
bitslicing, and requires few inputs to fill the registers. While
vslicing shines where arithmetic operations are needed, hslicing is
especially useful to implement ciphers that rely on intra-register
permutations, since those can be performed in a single shuffle
instruction. This includes Rectangle's permutations, which can be seen
as left-rotations, or Piccolo's permutation which operates on 8 bits
[5].

Inter-register permutations however, will be very expensive, as they
will require manually extracting bits from different registers and
recombining them.


#### Hsliced Rectangle

On Rectangle, the 64-bit input is again seen as 4 times 16 bits, but
this time, each bit goes to a distinct packed element:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/hslicing_oneway_small.png">
</p>

Once again, there are unused bits in the registers, which can be
filled with subsequent inputs. The second input would go into the
second bits of each packed element:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/hslicing_twoway_small.png">
</p>

Like for vslicing, this is repeated until the registers are full,
which in the case of Rectangle on SSE requires a total of 8
inputs. Numbering the bits from 0 (most significant) to 63 (least
significant) can help visualize hslicing's data layout:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/hslicing_full_numbered-small.png">
</p>

Here, the 8-bit element labelled 0 contains the first bit of each of
the 8 inputs.

Rectangle Sbox, composed solely of bitwise instructions, can be
computed using the same instruction as the vsliced implementation. The
permutation however, can be done even more efficiently than in
vslicing, using shuffle instructions. For instance, the first
left-rotation can be done with the following shuffle:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/shuffleROTL1-small.png">
</p>




<!--
Slicings comparison
 - same basis: constant-time, data-parallelism, transpostion
 - strengths/weaknesses:
   + transposition
     * vslicing cheap
     * hslicing somewhat cheap
     * bitslicing expensive
   + register pressure
   + arithmetics
   + permutations
     * bitslicing: compile-time
     * hslicing: pshufb
   + parallelism required
     * bitslicing needs _a lot_ of data
   + CPU SIMD instructions
     * mslicing only with SIMD instructions
-->

## Bitslicing _vs_ vslicing _vs_ hslicing


**Terminology**. Whenever the slicing direction (_i.e._ vertical or
horizontal) is unimportant, we talk about _m-slicing_ (assuming _m_ > 1)
and we call _slicing_ the technique encompassing both bitslicing and
_m_-slicing.

<!-- Note: in PhD thesis: remove this sentence and incorporate it in
the next one -->
The following picture summarizes all slicing forms:


<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/slicings_small.png">
</p>

All those slicing forms share the same basic properties: they are
constant-time, they rely on data-parallelism to increase throughput,
and they use some sort of tranposition to convert the data into a
layout suitable to their model.

However, each slicing form has its own strenghts and weaknesses:

 - transposing data to hslicing or vslicing usually has a cost almost
   negligible compared to a full cipher, while a bitslice
   transposition is much more expensive.
   
 - bitslicing introduces a lot of spilling, thus reducing its
   potential performances, unlike hslicing and vslicing.
   
 - only vslicing is a viable option on ciphers using arithmetic
   operations, since it can use CPU SIMD arithmetic instructions. As
   shown in the [previous post]({{ site.baseurl }}{% post_url
   2020-01-14-bitslicing %}), trying implement those instructions in
   bitslicing (and hslicing) does not yield good performances.
   
 - bitslicing is much better at doing permutations than both hslicing
   and vslicing, since it can do any [permutation at compile-time]({{
   site.baseurl }}{% post_url 2020-01-14-bitslicing
   %}#compile-time-permutations). Intra-register permutations can
   still be done efficiently (but at run time) in hslicing using SIMD
   shuffle instructions. Vslicing will only be a reasonable option in
   the absence of permutations, or when permutations can be easily
   expressed using rotations or shifts (like Rectangle's permutation
   for instance, which can be written as 3 rotations).
   
 - bitslicing requires much more data to provide a high-throughput
   than hslicing and vslicing. On Rectangle for instance, 8
   independent inputs are required to maximize the throughput on SSE
   registers using <i>m</i>slicing, while 128 inputs are required when
   using bitslicing.

 - both vslicing and hslicing rely on vector instructions, and are
   thus only available on CPUs with SIMD extensions, while bitslicing
   does not require any hardware-specific.

<!-- 
Conclusion
 - slicing choice depends on cipher & architecture
 - DES: bitslice
 - Chacha20: vslice
 - AES: hslice
 - Rectangle:
   + SIMD: vslice
   + no-SIMD: bitslice
   + no transpo: vslice/hslice
-->

The choice between bitslicing, vslicing and hslicing thus depends on
both the cipher and the target architecture. For instance, on CPUs
with SIMD extensions, the fastest implementations of DES are
bitsliced, the fastest implementations of Chacha20 are vsliced, while
the fastest AES implementations are hsliced. Even for a given cipher,
some slicings might be faster depending on the architecture: on
X86-64, the fastest implementation of Rectangle is bitsliced, while on
AVX, vslicing is ahead of both bitslicing and hslicing. If we exclude
the cost of transposing the data, then hsliced and vsliced
implementations of Rectangle have the same performances.


Manually estimating which slicing will be the most efficient for a
given cipher can often be complicated, especially on complex CPU
architectures. Instead, Usuba allows to write codes which are
polymorphic on the slicing types (when possible), making it easy to
switch from a slicing form to another. The developper can thus run
actual sliced implementations rather than guess which one would be
faster.



---

## References

[1] D. J. Bernstein, [Chacha, a variant of Salsa20](https://cr.yp.to/chacha/chacha-20080128.pdf), 2008.

[2] N. Ferguson _et al._, [The Skein Hash Function Family](https://www.schneier.com/academic/paperfiles/skein1.3.pdf), 2010.

[3] E. KÃ¤sper, P. Schwabe, [Faster and Timing-Attack Resistant AES-GCM](https://www.esat.kuleuven.be/cosic/publications/article-1261.pdf), CHES, 2009.

[4] W. Zhang _et al_, [RECTANGLE: A Bit-slice Lightweight Block Cipher Suitable for Multiple Platforms](https://eprint.iacr.org/2014/084.pdf), 2014.

[5] K. Shibutani _et al._, [Piccolo: An Ultra-Lightweight Blockcipher](https://www.iacr.org/archive/ches2011/69170343/69170343.pdf), CHES, 2011.
